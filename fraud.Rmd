---
title: "fraud"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Memory error
#options(java.parameters = "-Xmx16g")
options(java.parameters = c("-XX:+UseConcMarkSweepGC", "-Xmx8192m"))


### Read file
df = read.csv('F:/OneDrive - University Of Houston/Study/11th semester/MATH 4322/GP/creditcard.csv')

### Drop Time column
df$Time = NULL

### Scale Amount column
df$Amount = scale(df$Amount)

### Check for missing values
```{r}
colSums(is.na(df))
```

### Plot histograms of all predictors to check if they are normalized
par(mfrow = (c(4,8)))
for (col in 1:ncol(df[, -30])) {
  hist(df[, col], main = colnames(df)[col])}

### Convert response variable Class to factors
df$Class = factor(df$Class, levels = rev(levels(factor(df$Class))))
levels(df$Class) = c("Fraud", "NonFraud")

### Check for Class distribution
```{r}
prop.table(table(df$Class))
```

###########################
<!-- install.packages("snow") -->
<!-- install.packages("doSNOW", repos="http://R-Forge.R-project.org") -->
<!-- install.packages("caret") -->
<!-- install.packages("ROSE") -->
<!-- install.packages("PRROC") -->
<!-- install.packages("rpart") -->
<!-- install.packages("ranger") -->

library(ranger)
library(rpart)
library(caret) # for cross validation training and hyper-parameter tuning
library(doSNOW) # for parallel computing
library(PRROC) # for evaluation metrics
###########################
# Validation Set Approach
### Stratified split the dataset
set.seed(1)
cvIndex = createFolds(df$Class, k = 5, returnTrain = T)

### Get row index for training set
train = cvIndex$Fold1

### Create training, test sets
```{r}
X.train = df[train,]
X.test = df[-train,]
table(X.train$Class)
table(X.test$Class)
```

## Train classification tree
```{r}
tree.fit = rpart(Class ~ ., data = X.train, method = "class")
summary(tree.fit)
plot(tree.fit)
text(tree.fit, pretty = 0)
```

### Confusion Matrix
```{r}
confusionMatrix(predict(tree.fit, newdata = X.test, type = "class"), X.test$Class, positive = 'Fraud')  
```

### Tree PRAUC
```{r}
fg_pred = predict(tree.fit, newdata = X.test, type = "prob")[, 1]
bg_pred = predict(tree.fit, newdata = X.test, type = "prob")[, 2]
fg = fg_pred[X.test$Class == "Fraud"]
bg = bg_pred[X.test$Class == "NonFraud"]
x = c(-fg, -bg)
lab = c(rep(1, length(fg)), rep(0, length(bg)))
c = pr.curve(scores.class0 = x, weights.class0 = lab, curve=TRUE)
c
plot(c)
```

### Pruned tree does not make much difference, you can just skip this
```{r}
cvtree.fit = cv.tree(tree.fit, FUN = prune.misclass)
plot(cvtree.fit$size, cvtree.fit$dev, type = "b", xlab = "Tree size", ylab = "CV Error Rate")
pruned.fit = prune.misclass(tree.fit, best = 5)
summary(pruned.fit)
confusionMatrix(predict(pruned.fit, newdata = X.test, type = "class"), X.test$Class, positive = 'Fraud')  
```

## Train logistic regression model (using same training, test set)
```{r}
logreg <- glm(Class ~ . , family = "binomial", data = X.train)
predlg = predict(logreg, newdata = X.test, type = "response")
lg_pred = ifelse(predlg <= 0.5, "Fraud", "NonFraud")
confusionMatrix(factor(lg_pred), X.test$Class, positive = 'Fraud')
```

### Logistic Regression PRAUC
```{r}
fg = predlg[X.test$Class == "Fraud"]
bg = predlg[X.test$Class == "NonFraud"]
x = c(-fg, -bg)
lab = c(rep(1, length(fg)), rep(0, length(bg)))
c = pr.curve(scores.class0 = x, weights.class0 = lab, curve=TRUE)
plot(c)
```

## Random Forest
```{r}
rf = ranger(Class ~ ., data = X.train, mtry = 5, probability = TRUE, num.trees = 500)
rf_pred = predict(rf, data = X.test)
fg_pred = rf_pred$predictions[, 1]
bg_pred = rf_pred$predictions[, 2]
fg = fg_pred[X.test$Class == "Fraud"]
bg = bg_pred[X.test$Class == "NonFraud"]
x = c(-fg, -bg)
lab = c(rep(1, length(fg)), rep(0, length(bg)))
c = pr.curve(scores.class0 = x, weights.class0 = lab, curve=TRUE)
plot(c)
```
```{r}
temp = ifelse(rf_pred$predictions[, 1] > 0.5, "Fraud", "NonFraud")
confusionMatrix(factor(temp), X.test$Class, positive = 'Fraud')
```

# ```{r}
# mySummary  <- function(data, lev = NULL, model = NULL){
#   temp = data$Fraud
#   print(lev)
#   a <- prSummary(data, lev, model)
#   out <- a
#   out}
# ```

#################################
# Stratified 5-fold Cross Validation approach
### Create folds
set.seed(1)
cvIndex = createFolds(df$Class, k = 5, returnTrain = T)

### Training control
trControl = trainControl(index = cvIndex, method = "cv", classProbs = TRUE, summaryFunction = prSummary, allowParallel = TRUE, verboseIter = TRUE, savePredictions = TRUE, search = "random")

### Run workers
cl = makeCluster(8, outfile="")
registerDoSNOW(cl)

### Train the model
set.seed(1)
NN = train(Class ~ ., data = df, method = "avNNet", tuneLength = 10, metric = "AUC", trControl = trControl, verbose = TRUE)

plot(NN, metric="AUC")



### Stop workers and output
stopCluster(cl)
NN


# GLM
GLM = train(Class ~ ., data = df, method = "glm", metric = "AUC", trControl = trControl)

# RF
trControl = trainControl(index = cvIndex, method = "cv", classProbs = TRUE, summaryFunction = prSummary, allowParallel = FALSE, verboseIter = TRUE, savePredictions = TRUE, indexFinal = 1:1000)
RF = train(Class ~ ., data = df, method = "ranger", num.trees = 500, importance = 'impurity', metric = "AUC", trControl = trControl, verbose = TRUE, tuneGrid = expand.grid(mtry = 29, splitrule = "gini", min.node.size = 1))
varImp(RF)

# Bagged
Bagged = train(Class ~ ., data = df, method = "treebag", metric = "AUC", trControl = trControl, verbose = TRUE, control = rpart.control(cp = 0, ))

# Decision tree with cp
DT = train(Class ~ ., data = df, method = "rpart2", tuneLength = 20, metric = "AUC", trControl = trControl, control = rpart.control(xval = 10, minsplit = 1, minbucket = 1, cp = 0))

# NN
NN = train(Class ~ ., data = df, method = "nnet", metric = "AUC", trControl = trControl, tuneLength = 50, verbose = TRUE)

DDT
results <- resamples(list(GLM = GLM, DT = DT))
bwplot(results)



