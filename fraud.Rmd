---
title: "fraud"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Memory error
#options(java.parameters = "-Xmx16g")
options(java.parameters = c("-XX:+UseConcMarkSweepGC", "-Xmx8192m"))


### Read file
df = read.csv('F:/OneDrive - University Of Houston/Study/11th semester/MATH 4322/GP/creditcard.csv')

### Drop Time column
df$Time = NULL

### Scale Amount column
df$Amount = scale(df$Amount)

### Check for missing values
```{r}
colSums(is.na(df))
```

### Plot histograms of all predictors to check if they are normalized
par(mfrow = (c(4,8)))
for (col in 1:ncol(df[, -30])) {
  hist(df[, col], main = colnames(df)[col])}

### Convert response variable Class to factors
df$Class = factor(df$Class, levels = rev(levels(factor(df$Class))))
levels(df$Class) = c("Fraud", "NonFraud")

### Check for Class distribution
```{r}
prop.table(table(df$Class))
```

############################################################
<!-- install.packages("snow") -->
<!-- install.packages("doSNOW", repos="http://R-Forge.R-project.org") -->
<!-- install.packages("caret") -->
<!-- install.packages("ROSE") -->
<!-- install.packages("PRROC") -->
<!-- install.packages("rpart") -->
<!-- install.packages("ranger") -->
<!-- install.packages("rpart.plot") -->

library(ranger)
library(rpart)
library(caret) # for cross validation training and hyper-parameter tuning
library(doSNOW) # for parallel computing
library(PRROC) # for evaluation metrics
############################################################
# Validation Set Approach
### Stratified split the dataset
set.seed(1)
cvIndex = createFolds(df$Class, k = 5, returnTrain = T)

### Get row index for training set
train = cvIndex$Fold1

### Create training, test sets
```{r}
X.train = df[train,]
X.test = df[-train,]
table(X.train$Class)
table(X.test$Class)
```

## Train classification tree
```{r}
tree.fit = rpart(Class ~ ., data = X.train, method = "class")
summary(tree.fit)
rpart.plot::rpart.plot(tree.fit)
```

### Confusion Matrix
```{r}
confusionMatrix(predict(tree.fit, newdata = X.test, type = "class"), X.test$Class, positive = 'Fraud')  
```

### Tree PRAUC
```{r}
fg_pred = predict(tree.fit, newdata = X.test, type = "prob")[, 1]
bg_pred = predict(tree.fit, newdata = X.test, type = "prob")[, 2]
fg = fg_pred[X.test$Class == "Fraud"]
bg = bg_pred[X.test$Class == "NonFraud"]
x = c(-fg, -bg)
lab = c(rep(1, length(fg)), rep(0, length(bg)))
c = pr.curve(scores.class0 = x, weights.class0 = lab, curve=TRUE)
c
plot(c)
```

### Pruned tree does not make much difference, you can just skip this
```{r}
cvtree.fit = cv.tree(tree.fit, FUN = prune.misclass)
plot(cvtree.fit$size, cvtree.fit$dev, type = "b", xlab = "Tree size", ylab = "CV Error Rate")
pruned.fit = prune.misclass(tree.fit, best = 5)
summary(pruned.fit)
confusionMatrix(predict(pruned.fit, newdata = X.test, type = "class"), X.test$Class, positive = 'Fraud')  
```

### Train logistic regression model (using same training, test set)
```{r}
logreg <- glm(Class ~ . , family = "binomial", data = X.train)

predlg = predict(logreg, newdata = X.test, type = "response")
lg_pred = ifelse(predlg <= 0.9, "Fraud", "NonFraud")
confusionMatrix(factor(lg_pred), X.test$Class, positive = 'Fraud')
```

### Logistic Regression PRAUC
```{r}
fg = predlg[X.test$Class == "Fraud"]
bg = predlg[X.test$Class == "NonFraud"]
x = c(-fg, -bg)
lab = c(rep(1, length(fg)), rep(0, length(bg)))
c = pr.curve(scores.class0 = x, weights.class0 = lab, curve=TRUE)
plot(c)
```

## Random Forest
```{r}
rf = ranger(Class ~ ., data = X.train, mtry = 5, probability = TRUE, num.trees = 500)
rf_pred = predict(rf, data = X.test)
fg_pred = rf_pred$predictions[, 1]
bg_pred = rf_pred$predictions[, 2]
fg = fg_pred[X.test$Class == "Fraud"]
bg = bg_pred[X.test$Class == "NonFraud"]
x = c(-fg, -bg)
lab = c(rep(1, length(fg)), rep(0, length(bg)))
c = pr.curve(scores.class0 = x, weights.class0 = lab, curve=TRUE)
plot(c)
```
```{r}
temp = ifelse(rf_pred$predictions[, 1] > 0.5, "Fraud", "NonFraud")
confusionMatrix(factor(temp), X.test$Class, positive = 'Fraud')
```

############################################################
# Stratified 10-fold Cross Validation approach
### Create folds
set.seed(1)
cvIndex = createFolds(df$Class, k = 10, returnTrain = T)

### Training control
trControl = trainControl(index = cvIndex, method = "cv", returnData = FALSE, classProbs = TRUE, summaryFunction = prSummary, allowParallel = TRUE, verboseIter = TRUE, savePredictions = TRUE)

### Run workers for parallel computing
cl = makeCluster(8, outfile="")
registerDoSNOW(cl)

### Train model: Select a model below and paste the code here.
set.seed(1)
model = train(.................)

### Stop workers
stopCluster(cl)

## Tuned Decision tree fully grown
DT_tuned_no_cp = train(Class ~ ., data = df, method = "rpart", metric = "AUC", trControl = trControl, tuneGrid = expand.grid(cp = 0), control = rpart.control(minsplit = 22, minbucket = 7, cp = 0))

## Tuned Decision tree hyperparameter tuning for best cp
DT_tuned_cp_search = train(Class ~ ., data = df, method = "rpart", metric = "AUC", trControl = trControl, tuneGrid = expand.grid(cp = runif(200, 0, 0.01)), control = rpart.control(minsplit = 22, minbucket = 7, cp = 0))

## Tuned Decision tree with optimal cp
DT_tuned_cp = train(Class ~ ., data = df, method = "rpart", metric = "AUC", trControl = trControl, tuneGrid = expand.grid(cp = 0.002186453), control = rpart.control(minsplit = 22, minbucket = 7))

## Bagged tree
Bagged = train(Class ~ ., data = df, method = "ranger", num.trees = 500, importance = 'impurity', metric = "AUC", trControl = trControl, verbose = TRUE, tuneGrid = expand.grid(mtry = 29, splitrule = "gini", min.node.size = 10))

## RF
RF = train(Class ~ ., data = df, method = "ranger", num.trees = 500, importance = 'impurity', metric = "AUC", trControl = trControl, verbose = TRUE, tuneGrid = expand.grid(mtry = 6, splitrule = "gini", min.node.size = 10))

## GLM
GLM = train(Class ~ ., data = df, method = "glm", metric = "AUC", trControl = trControl, family = "binomial")

## NN
NN = train(Class ~ ., data = df, method = "nnet", maxit = 200, metric = "AUC", trControl = trControl, verbose = TRUE, tuneGrid = expand.grid(size = 14, decay = 1.25))

### Save model
save(GLM, file = "GLM.Rdata")

### Load model
load("GLM.Rdata")

### Compare model
results <- resamples(list(GLM = GLM, DT = DT, NN = NN))
bwplot(results)